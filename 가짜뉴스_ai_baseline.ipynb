{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "가짜뉴스 ai_baseline",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1yXB-nlIg8ZxeoxKsdEUtzeYY83-Odfae",
      "authorship_tag": "ABX9TyPLq4ypixihq6Jknlkv+cIf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gihoonpark/Fake_News_classification-NLP/blob/main/%EA%B0%80%EC%A7%9C%EB%89%B4%EC%8A%A4_ai_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKq426awvx_g"
      },
      "source": [
        "Setting Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4r7EClevm4u"
      },
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jdt3LA0zv20F"
      },
      "source": [
        "Data load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6vnZSyGOxb7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from konlpy.tag import Mecab\n",
        "from pandas import concat\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPkThCCBwn-8"
      },
      "source": [
        "df1 = pd.read_csv('/content/drive/MyDrive/dataset/가짜뉴스ai/news_train.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/dataset/가짜뉴스ai/news_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFk9W4mWrONN"
      },
      "source": [
        "train_df = df1.loc[:, ['title','content','info','ord']]\n",
        "test_df = df2.loc[:, ['title','content','ord']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40rnvqymwBOs"
      },
      "source": [
        "Data cleaning & tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR70KPUSwOqO"
      },
      "source": [
        "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', \n",
        "             '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게', '만', '되', '음', '면']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp4SEjn0wMpa"
      },
      "source": [
        "train_df['title'] = train_df['title'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣0-9%$ ]\",\"\") # 정규 표현식 수행\n",
        "train_df['content'] = train_df['content'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣0-9%$ ]\",\"\") # 정규 표현식 수행\n",
        "train_df[['title','content']].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "train_df[['title','content']] = train_df[['title','content']].dropna(how='any') # Null 값 제거\n",
        "\n",
        "\n",
        "test_df['title'] = test_df['title'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣0-9%$ ]\",\"\") # 정규 표현식 수행\n",
        "test_df['content'] = test_df['content'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣0-9%$ ]\",\"\") # 정규 표현식 수행\n",
        "test_df[['title','content']].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "test_df[['title','content']] = test_df[['title','content']].dropna(how='any') # Null 값 제거"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn6UV_C4vsTB"
      },
      "source": [
        "mecab = Mecab()\n",
        "train_df['title'] = train_df['title'].apply(mecab.morphs)\n",
        "train_df['content'] = train_df['content'].apply(mecab.morphs)\n",
        "train_df['title'] = train_df['title'].apply(lambda x: [item for item in x if item not in stopwords])\n",
        "train_df['content'] = train_df['content'].apply(lambda x: [item for item in x if item not in stopwords])\n",
        "\n",
        "test_df['title'] = test_df['title'].apply(mecab.morphs)\n",
        "test_df['content'] = test_df['content'].apply(mecab.morphs)\n",
        "test_df['title'] = test_df['title'].apply(lambda x: [item for item in x if item not in stopwords])\n",
        "test_df['content'] = test_df['content'].apply(lambda x: [item for item in x if item not in stopwords])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJUDYV_CxmiL"
      },
      "source": [
        "title_tokenizer = Tokenizer(14000)\n",
        "title_tokenizer.fit_on_texts(train_df['title'])\n",
        "\n",
        "content_tokenizer = Tokenizer(160000)\n",
        "content_tokenizer.fit_on_texts(train_df['content'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3oMcjK4FOgQ"
      },
      "source": [
        "title_sequences = title_tokenizer.texts_to_sequences(train_df['title'])\n",
        "content_sequences = content_tokenizer.texts_to_sequences(train_df['content'])\n",
        "\n",
        "test_title_sequences = title_tokenizer.texts_to_sequences(test_df['title'])\n",
        "test_content_sequences = content_tokenizer.texts_to_sequences(test_df['content'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGmaz6D-FIDI"
      },
      "source": [
        "# title, content 단어 수\n",
        "len(title_tokenizer.word_index), len(content_tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY0YqUiJ3w2C"
      },
      "source": [
        "# title 시퀀스 수\n",
        "print('리뷰의 최대 길이 :',max(len(l) for l in train_df['title']))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, train_df['title']))/len(train_df['title']))\n",
        "plt.hist([len(s) for s in train_df['title']], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axEkmgc0C039"
      },
      "source": [
        "# content 시퀀스 수\n",
        "print('리뷰의 최대 길이 :',max(len(l) for l in train_df['content']))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, train_df['content']))/len(train_df['content']))\n",
        "plt.hist([len(s) for s in train_df['content']], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-fW0P5AGKep"
      },
      "source": [
        "# test_title 시퀀스 수\n",
        "print('리뷰의 최대 길이 :',max(len(l) for l in test_df['title']))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, test_df['title']))/len(test_df['title']))\n",
        "plt.hist([len(s) for s in test_df['title']], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCApidxvGKkn"
      },
      "source": [
        "# test_content 시퀀스 수\n",
        "print('리뷰의 최대 길이 :',max(len(l) for l in test_df['content']))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, test_df['content']))/len(test_df['content']))\n",
        "plt.hist([len(s) for s in test_df['content']], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYBT53NA4F3h"
      },
      "source": [
        "padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXL8ly9A3ipK"
      },
      "source": [
        "X_train1 = pad_sequences(title_sequences, maxlen = 50, padding='post')\n",
        "X_train2 = pad_sequences(content_sequences, maxlen = 200, padding='post')\n",
        "y_train = train_df['info']\n",
        "y_train = y_train.values.reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGxK5qEBcCm8"
      },
      "source": [
        "X_test1 = pad_sequences(test_title_sequences, maxlen = 50, padding='post')\n",
        "X_test2 = pad_sequences(test_content_sequences, maxlen = 200, padding='post')\n",
        "\n",
        "X_train1.shape, X_train2.shape, y_train.shape, X_test1.shape, X_test2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-BXKNr2_OXl"
      },
      "source": [
        "X_train3 = train_df['ord'].values.reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSAjYszA_OWz"
      },
      "source": [
        "X_test3 = test_df['ord'].values.reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxf3WNuBht5X"
      },
      "source": [
        "train, validation split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f2WgC-qhtP4"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train1, X_val1, X_train2, X_val2, X_train3, X_val3, y_train, y_val = train_test_split(X_train1, X_train2,  X_train3, y_train, test_size=0.3, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fQrQ4vWhWm3"
      },
      "source": [
        "n_fold = 5\n",
        "seed = 42\n",
        "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es7tCaQSxm3v"
      },
      "source": [
        "Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLl9aZaixpXb"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout, Input, Flatten, BatchNormalization, Reshape, Concatenate, Conv1D,GRU\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRSQDnpzxpZz"
      },
      "source": [
        "def build_model1():\n",
        "    \n",
        "    title_input = Input(shape=(50,))\n",
        "    title_embed = Embedding(14000, 516, input_length=50)(title_input)\n",
        "    title_lstm1 = Bidirectional(LSTM(256))(title_embed)\n",
        "    title_flat = Flatten()(title_lstm1)\n",
        "    title_output = Dense(32)(title_flat)\n",
        "    \n",
        "    content_input = Input(shape=(200,))\n",
        "    content_embed = Embedding(160000, 516, input_length=200)(content_input)\n",
        "    content_lstm1 = Bidirectional(LSTM(256))(content_embed)\n",
        "    content_flat = Flatten()(content_lstm1)\n",
        "    content_output = Dense(256)(content_flat)\n",
        "\n",
        "    concatenated = Concatenate()([title_output, content_output])\n",
        "    batchnorm = BatchNormalization()(concatenated)\n",
        "    dense = Dense(128, activation='relu')(batchnorm)\n",
        "    final_output = Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "    model = Model(inputs=[title_input, content_input], outputs=final_output)\n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR_2jfNkhZ5g"
      },
      "source": [
        "def build_model2():\n",
        "    \n",
        "    title_input = Input(shape=(50,))\n",
        "    title_embed = Embedding(14000, 516, input_length=50)(title_input)\n",
        "    title_lstm1 = Bidirectional(LSTM(256))(title_embed)\n",
        "    title_flat = Flatten()(title_lstm1)\n",
        "    title_output = Dense(32)(title_flat)\n",
        "    title_dropout = Dropout(0.4)(title_output)\n",
        "    \n",
        "    content_input = Input(shape=(200,))\n",
        "    content_embed = Embedding(160000, 516, input_length=200)(content_input)\n",
        "    content_lstm1 = Bidirectional(LSTM(256))(content_embed)\n",
        "    content_flat = Flatten()(content_lstm1)\n",
        "    content_output = Dense(256)(content_flat)\n",
        "    content_dropout = Dropout(0.4)(content_output)\n",
        "\n",
        "    ord_input = Input(shape=(1,))\n",
        "    ord_dense = Dense(32)(ord_input)\n",
        "\n",
        "    concatenated = Concatenate()([title_dropout, content_dropout, ord_dense])\n",
        "    batchnorm1 = BatchNormalization()(concatenated)\n",
        "    dense1 = Dense(128, activation='relu')(batchnorm1)\n",
        "    batchnorm2 = BatchNormalization()(dense1)\n",
        "    dense2 = Dense(64, activation='relu')(batchnorm2)\n",
        "    final_output = Dense(1, activation='sigmoid')(dense2)\n",
        "\n",
        "    model = Model(inputs=[title_input, content_input, ord_input], outputs=final_output)\n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    \n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQyxawIIs9Zs"
      },
      "source": [
        "def LSTM_model():\n",
        "    \n",
        "    title_input = Input(shape=(30,))\n",
        "    title_embed = Embedding(10000, 256, input_length=30)(title_input)\n",
        "    title_lstm1 = Bidirectional(LSTM(128, return_sequences=True))(title_embed)\n",
        "    title_lstm2 = Bidirectional(LSTM(128))(title_lstm1)\n",
        "    title_flat = Flatten()(title_lstm2)\n",
        "    title_output = Dense(10)(title_flat)\n",
        "    \n",
        "    content_input = Input(shape=(200,))\n",
        "    content_embed = Embedding(45000, 256, input_length=200)(content_input)\n",
        "    content_lstm1 = Bidirectional(LSTM(128, return_sequences=True ))(content_embed)\n",
        "    content_lstm2 = Bidirectional(LSTM(128))(content_lstm1)\n",
        "    content_flat = Flatten()(content_lstm2)\n",
        "    content_output = Dense(10)(content_flat)\n",
        "\n",
        "    concatenated = Concatenate()([title_output, content_output])\n",
        "    batchnorm = BatchNormalization()(concatenated)\n",
        "    final_output = Dense(1, activation='sigmoid')(batchnorm)\n",
        "\n",
        "    model = Model(inputs=[title_input, content_input], outputs=final_output)\n",
        "    \n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tLDj2gtHy8N"
      },
      "source": [
        "# build_model1 (two input : title, content)\n",
        "# val_score = 0.9761, private_score = 0.9438878534 (12.02)\n",
        "from keras.utils import plot_model\n",
        "es = EarlyStopping(monitor='accuracy', mode='max', verbose=1, patience = 3, restore_best_weights=False)\n",
        "annealer = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** x)\n",
        "model1 = build_model1()\n",
        "#plot_model(model, show_shapes=True)\n",
        "model1.fit([X_train1, X_train2], y_train, epochs = 30, batch_size = 256, validation_data=([X_val1, X_val2], y_val), verbose=1, callbacks=[es, annealer])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJzt6KU_DyyW"
      },
      "source": [
        "# build_model2 (three input : title, content, ordinary)\n",
        "# val_score = 0.9814, private_score = 0.9648182665 (12.02), select this model\n",
        "es = EarlyStopping(monitor='accuracy', mode='max', verbose=1, patience = 3, restore_best_weights=True)\n",
        "annealer = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** x)\n",
        "model2 = build_model2()\n",
        "model2.fit([X_train1, X_train2,  X_train3], y_train, epochs = 30, batch_size = 256, validation_data=([X_val1, X_val2, X_val3], y_val), verbose=1, callbacks=[es])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkWQ4GnjKlnu"
      },
      "source": [
        "# LSTM_model (two input + add more LSTM layers)\n",
        "es = EarlyStopping(monitor='accuracy', mode='max', verbose=1, patience = 3, restore_best_weights=True)\n",
        "annealer = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** x)\n",
        "model3 = LSTM_model()\n",
        "model3.fit([X_train1, X_train2], y_train, epochs = 30, batch_size = 256, validation_data=([X_val1, X_val2], y_val), verbose=1, callbacks=[es])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2mIUWaBSu1x"
      },
      "source": [
        "\r\n",
        "Predict test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBhnZTP9VF-Y"
      },
      "source": [
        "sub_df = model.predict([X_test1, X_test2, X_test3]) # build_model2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug71-4C8kpB3"
      },
      "source": [
        "sub_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Tvrs5GMfbXO"
      },
      "source": [
        "submission = []\n",
        "for i in sub_df:\n",
        "    if i < 0.5:\n",
        "        submission.append(0)\n",
        "    else:\n",
        "        submission.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8aErpxkcMQz"
      },
      "source": [
        "sum(submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4CmPyO1VRus"
      },
      "source": [
        "sub = pd.read_csv('/content/drive/MyDrive/dataset/가짜뉴스ai/sample_submission.csv', index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIYbbqZxqabt"
      },
      "source": [
        "sub['info'] = submission\n",
        "sub.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhlZozCqVRxV"
      },
      "source": [
        "sub.to_csv('/content/drive/MyDrive/dataset/가짜뉴스ai/sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}